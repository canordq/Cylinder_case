{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a68f6c38",
   "metadata": {},
   "source": [
    "# DETERMINISTIC_RUN_MARL\n",
    "Exploitation --> No longer exploring!\n",
    "See if model is working properly or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd3d98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/env python\n",
    "#\n",
    "# DEEP REINFORCEMENT LEARNING WITH ALYA\n",
    "#\n",
    "# Deterministic run launcher\n",
    "#\n",
    "# Pol Suarez, Francisco Alcantara\n",
    "# 21/02/2022\n",
    "import os, time, sys\n",
    "import copy as cp\n",
    "\n",
    "from tensorforce.agents import Agent\n",
    "from tensorforce.execution import Runner \n",
    "\n",
    "from env_utils  import run_subprocess, generate_node_list, read_node_list\n",
    "\n",
    "from coordinate_folders_deterministic import adjust_folders #TODO: what is this??\n",
    "\n",
    "from cr import cr_report, cr_start, cr_stop\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "cr_start('deterministic-100-3D',0)\n",
    "\n",
    "# Run the cleaner\n",
    "#run_subprocess('./',ALYA_ULTCL,'') \n",
    "\n",
    "# Set up which case to run\n",
    "training_case = \"cylinder_3D_MARL\"  # cylinder_2D, airfoil_2D\n",
    "run_subprocess('./','rm -f','parameters.py') # Ensure deleting old parameters\n",
    "run_subprocess('./','cp','parameters/parameters_{}.py parameters.py'.format(training_case))\n",
    "run_subprocess('alya_files','cp -r','case_{} case'.format(training_case))\n",
    "\n",
    "runbin  = 'rm -rf'\n",
    "path    = 'alya_files'\n",
    "args    = 'environment*'\n",
    "run_subprocess(path,runbin,args)\n",
    "\n",
    "runbin  = 'rm -rf'\n",
    "path    = './'\n",
    "args    = 'best_model saved_models rewards final_rewards actions'\n",
    "run_subprocess(path,runbin,args)\n",
    "\n",
    "from Env3D_MARL      import Environment\n",
    "from parameters import nb_inv_per_CFD, sync_episodes, batch_size, nb_actuations, nb_actuations_deterministic, num_episodes, num_servers, nb_proc, simu_name, run_baseline, nz_Qs, optimization_params,output_params, neighbor_state\n",
    "\n",
    "from threading import Thread\n",
    "from multiprocessing import Process\n",
    "\n",
    "from witness          import read_last_wit\n",
    "\n",
    "\n",
    "## Run\n",
    "initial_time = time.time()\n",
    "\n",
    "# Generate the list of nodes\n",
    "# TODO: hardcoded for 1 node/env???\n",
    "generate_node_list(num_servers=num_servers,num_cores_server=nb_proc) \n",
    "\n",
    "# Read the list of nodes\n",
    "nodelist = read_node_list()\n",
    "\n",
    "#TODO: assign more nodes to an environment\n",
    "environment_base = Environment(simu_name = simu_name, node=nodelist[1], deterministic=True) \n",
    "#Check parameters to run_baseline flag\n",
    "if run_baseline: environment_base.run_baseline(True)\n",
    "\n",
    "# Load agent TensorFlow checkpoint\n",
    "agent = Agent.load(directory=os.path.join(os.getcwd(), 'saver_data'), format='checkpoint', environment=environment_base)\n",
    "\n",
    "def list_observation(probes_values_global, ENV_ID, nb_inv_per_CFD):\n",
    "\n",
    "    if not neighbor_state:\n",
    "        #TODO: filter this observation state to each invariant and its neighbours: \n",
    "        batch_size_probes = int(len(probes_values_global)/nb_inv_per_CFD)\n",
    "        probes_values_2 = probes_values_global[((ENV_ID[1]-1)*batch_size_probes):(ENV_ID[1]*batch_size_probes)]\n",
    "    \n",
    "    else:\n",
    "        #TODO: filter this observation state to each invariant and its neighbours: \n",
    "        batch_size_probes = int(len(probes_values_global)/nb_inv_per_CFD)\n",
    "    \n",
    "        if ENV_ID[1] == 1:\n",
    "            probes_values_halo  = probes_values_global[((nb_inv_per_CFD-1)*batch_size_probes):(nb_inv_per_CFD*batch_size_probes)]\n",
    "            probes_values       = probes_values_global[((ENV_ID[1]-1)*batch_size_probes):((ENV_ID[1]+1)*batch_size_probes)]\n",
    "            probes_values_2     = np.concatenate((probes_values_halo, probes_values))\n",
    "\n",
    "        elif ENV_ID[1] == nb_inv_per_CFD:\n",
    "            probes_values      = probes_values_global[((ENV_ID[1]-2)*batch_size_probes):(ENV_ID[1]*batch_size_probes)]\n",
    "            probes_values_halo = probes_values_global[0:batch_size_probes]\n",
    "            probes_values_2    = np.concatenate((probes_values, probes_values_halo))\n",
    "\n",
    "        else:\n",
    "            probes_values_2    = probes_values_global[((ENV_ID[1]-2)*batch_size_probes):((ENV_ID[1]+1)*batch_size_probes)]\n",
    "\n",
    "    return probes_values_2\n",
    "\n",
    "def split(environment, np):  # called 1 time in PARALLEL_TRAINING.py\n",
    "    # np:= number of the parallel environment. e.g. between [1,4] for 4 parallel CFDenvironments\n",
    "    # (ni, nj):= env_ID[1]:= 'position'/'ID-card' of the 'pseudo-parallel' invariant environment (a tuple in 3d, in which we have a grid of actuators. A scalar in 2D, in which we have a line of actuators)\n",
    "    # nb_inv_envs:= total number of 'pseudo-parallel' invariant environments. e.g. 10\n",
    "    ''' input: one of the parallel environments (np); output: a list of nb_inv_envs invariant environments identical to np. Their ID card: (np, ni)'''\n",
    "    list_inv_envs = []\n",
    "    for j in range(nz_Qs):\n",
    "        env = cp.copy(environment)\n",
    "        env.ENV_ID = [np, (j+1)]\n",
    "        env.host=\"environment{}\".format(np)\n",
    "        list_inv_envs.append(env)\n",
    "    return list_inv_envs\n",
    "\n",
    "### Here the array of environments is defined, will be n-1 host (the 1st one is MASTER) #TODO: assign more nodes to an environment:\n",
    "print('Here is the nodelist: ', nodelist)\n",
    "\n",
    "#here the array of environments is defined, will be n-1 nodes (the 1st one is MASTER) #TODO: assign more nodes to an environment\n",
    "parallel_environments = [Environment(simu_name = simu_name, deterministic=True, ENV_ID=[i,0], host=\"deterministic\", node=nodelist[i+1]) for i in range(num_servers)]\n",
    "\n",
    "environments = [split(parallel_environments[i], i+1)[j] for i in range(num_servers) for j in range(nz_Qs)]\n",
    "\n",
    "for env in environments:\n",
    "    print('Verif : ID ', env.ENV_ID, env.host)\n",
    "\n",
    "time.sleep(1.0)\n",
    "\n",
    "list_states = []\n",
    "\n",
    "for e in environments:\n",
    "    e.start()\n",
    "    list_states.append(e.reset())\n",
    "    time.sleep(0.2) \n",
    "\n",
    "# to run evaluation==True + multiprocessing/multienv \n",
    "for action_step in range(nb_actuations_deterministic):    \n",
    "    for e in reversed(environments):\n",
    "        #update action list --> merged   \n",
    "        list_actions = [agent.act(list_states[e.ENV_ID[1]-1], deterministic=True, independent = True)]\n",
    "        print('New action generated! ID ', env.ENV_ID, env.host, list_actions)\n",
    "        #update state + update BC --> becareful data race (nightmare)\n",
    "        list_states[e.ENV_ID[1]-1], terminal, reward = e.execute(list_actions[0])\n",
    "        print('CFD + action finished! ID ', env.ENV_ID, env.host)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2babdcd5",
   "metadata": {},
   "source": [
    "Tensorforce is made for MARL and doesn't understnad \"1 environment\" so we need to run all 8 and take only the first. Don't modify much here because it should work. It calls the tensorforce API directly using whole framework "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ca82b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #we need to rewrite manually the state of the non-CFD runners (execute was doing that for us but we need to split)\n",
    "    for e in environments:\n",
    "        if e.ENV_ID[1] != 1:            \n",
    "            # Read witness file from behind, last instant (FROM THE INVARIANT running [*,1])\n",
    "            nwit_to_read        = 1\n",
    "            filename             = os.path.join('alya_files','environment1','1','EP_1','cylinder.nsi.wit')\n",
    "\n",
    "            #read witness file and extract the entire array list\n",
    "            probes_values_global = read_last_wit(filename,output_params[\"probe_type\"], optimization_params[\"norm_press\"], nwit_to_read)\n",
    "\n",
    "            #filter probes per jet (corresponding to the ENV.ID[])\n",
    "            list_states[e.ENV_ID[1]-1] = list_observation(nb_inv_per_CFD=nb_inv_per_CFD, probes_values_global=probes_values_global,ENV_ID=e.ENV_ID)\n",
    "            print('non-CFD states updated! ID ', env.ENV_ID, env.host)\n",
    "\n",
    "agent.close()\n",
    "\n",
    "# Adjust folders for CONTINUE_LEARNING\n",
    "adjust_folders(num_servers)\n",
    "\n",
    "end_time = time.time()\n",
    "cr_stop('deterministic-100-3D',0)\n",
    "cr_report('deterministic.profile.csv')\n",
    "\n",
    "print(\"DRL deterministic simulation :\\nStart at : {}.\\nEnd at {}\\nDone in : {}\".format(initial_time,end_time,end_time-initial_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
